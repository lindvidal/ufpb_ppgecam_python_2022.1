{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29ba612",
   "metadata": {},
   "source": [
    "HDF5 for Python: https://docs.h5py.org/en/stable/\n",
    "<br>Pandas HDFStore: https://pandas.pydata.org/pandas-docs/stable/reference/io.html?highlight=hdfstore#hdfstore-pytables-hdf5\n",
    "<br>Compression (Additional Topic): https://www.youtube.com/watch?v=FfkLFQR_nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f18426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e880c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando dados aleatórios\n",
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))\n",
    "#print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeb41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#escrevendo um arquivo com mode = 'w' de writing\n",
    "with h5py.File('hdf5_data.h5', mode = 'w') as hdf:\n",
    "    hdf.create_dataset('dataset1', data = matrix1)\n",
    "    hdf.create_dataset('dataset2', data = matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2fe3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Datasets:\n",
      " ['dataset1', 'dataset2'] \n",
      "\n",
      "[[0.55278513 0.62240909 0.88230889 ... 0.50708217 0.95047539 0.48034496]\n",
      " [0.47246272 0.33327952 0.48406303 ... 0.31341004 0.02580535 0.94697971]\n",
      " [0.00217538 0.34710153 0.47126849 ... 0.22148122 0.33982791 0.0820403 ]\n",
      " ...\n",
      " [0.43101893 0.98445002 0.55166039 ... 0.55050911 0.16426889 0.33242244]\n",
      " [0.0148577  0.88182821 0.82763886 ... 0.01601607 0.9161697  0.6204427 ]\n",
      " [0.45737672 0.43420569 0.10360792 ... 0.00602328 0.30560827 0.39956688]]\n"
     ]
    }
   ],
   "source": [
    "#lendo um arquivo com mode = 'r' de reading, com fechamento automático\n",
    "with h5py.File('hdf5_data.h5', mode = 'r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('Lista de Datasets:\\n', ls,'\\n')\n",
    "#coletando o dado\n",
    "    data = hdf.get('dataset1')\n",
    "    dataset1 = np.array(data)\n",
    "    print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe798c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset1', 'dataset2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outra forma de ler arquivos\n",
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "ls = list(f.keys())\n",
    "#fechamento do arquivo manual\n",
    "f.close()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd197ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['dataset1', 'dataset4']>\n",
      "<KeysViewHDF5 ['SubGroup1', 'SubGroup2']>\n",
      "<KeysViewHDF5 ['dataset3']>\n",
      "<KeysViewHDF5 ['dataset2']>\n"
     ]
    }
   ],
   "source": [
    "#escrevendo um arquivo com grupos\n",
    "with h5py.File('hdf5_data.h5', mode = 'w') as hdf:\n",
    "    #criando grupo1 e objeto\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    #criando datasets dentro do grupo\n",
    "    G1.create_dataset('dataset1', data = matrix1)\n",
    "    G1.create_dataset('dataset4', data = matrix4)\n",
    "    \n",
    "    #criando grupo2 e objeto\n",
    "    G2 = hdf.create_group('Group2/SubGroup1')\n",
    "    #criando datasets dentro do grupo\n",
    "    G2.create_dataset('dataset3', data = matrix3)\n",
    "    \n",
    "    #criando grupo3 e objeto\n",
    "    G3 = hdf.create_group('Group2/SubGroup2')\n",
    "    #criando datasets dentro do grupo\n",
    "    G3.create_dataset('dataset2', data = matrix2)\n",
    "    \n",
    "    #checando datasets criados\n",
    "    print(hdf.get('Group1').keys())\n",
    "    print(hdf.get('Group2').keys())\n",
    "    print(hdf.get('Group2').get('SubGroup1').keys())\n",
    "    print(hdf.get('Group2').get('SubGroup2').keys())\n",
    "    #print(G3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(G3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6937753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Items no Root:\n",
      " [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)] \n",
      "\n",
      "Lista de Items no G1:\n",
      " [('dataset1', <HDF5 dataset \"dataset1\": shape (1000, 1000), type \"<f8\">), ('dataset4', <HDF5 dataset \"dataset4\": shape (1000, 1000), type \"<f8\">)] \n",
      "\n",
      "[[0.9517096  0.77198293 0.86786051 ... 0.39726706 0.29607245 0.57686556]\n",
      " [0.85993389 0.68411044 0.58089068 ... 0.32944731 0.53285263 0.5922828 ]\n",
      " [0.03714377 0.00659872 0.14969506 ... 0.7221112  0.1011124  0.08066508]\n",
      " ...\n",
      " [0.44405693 0.54165458 0.49125741 ... 0.43673065 0.58357507 0.09173127]\n",
      " [0.12679462 0.80637966 0.01953461 ... 0.66482085 0.7186889  0.23854753]\n",
      " [0.23944659 0.02765846 0.62226838 ... 0.04072119 0.39286183 0.30905399]]\n",
      "(1000, 1000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lendo grupos de um arquivo com mode = 'r' de reading\n",
    "with h5py.File('hdf5_data.h5', mode = 'r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Lista de Items no Root:\\n', base_items,'\\n')\n",
    "    G1 = hdf.get('Group1')\n",
    "    G1_items = list(G1.items())\n",
    "    print('Lista de Items no G1:\\n', G1_items,'\\n')\n",
    "    \n",
    "    #coletando dados de dataset\n",
    "    dataset4 = G1.get('dataset4')\n",
    "    table = np.array(dataset4)\n",
    "    print(table)\n",
    "    print(table.shape,'\\n')\n",
    "    #df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c79eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lendo grupos de um arquivo com mode = 'r' de reading\n",
    "with h5py.File('hdf5_data.h5', mode = 'r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Lista de Items no Root:\\n', base_items,'\\n')\n",
    "    \n",
    "    #listando items do grupo 2\n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Lista de Items no G2:\\n', G2_items,'\\n')\n",
    "    \n",
    "    #listando items do grupo 2, subgroup 1\n",
    "    G21 = G2.get('SubGroup1')\n",
    "    #G21 = hdf.get('/Group2/SubGroup1')\n",
    "    G21_items = list(G21.items())\n",
    "    print('Lista de Items no G2, SG1:\\n', G21_items,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando arquivo com atributos (jeito oficial de armazenar metadados)\n",
    "hdf = h5py.File('hdf5_data_metadata.h5', mode = 'w')\n",
    "\n",
    "#criando datasets dentro do grupo\n",
    "dataset1 = hdf.create_dataset('dataset1', data = matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data = matrix2)\n",
    "\n",
    "#determinando atributos\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = '1.1'\n",
    "\n",
    "#fechamento manual\n",
    "hdf.close()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877a73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Items no Root:\n",
      " ['dataset1', 'dataset2'] \n",
      "\n",
      "['CLASS', 'VERSION']\n",
      "['DATA MATRIX', '1.1']\n",
      "['CLASS', 'VERSION']\n",
      "['DATA MATRIX', '1.1']\n"
     ]
    }
   ],
   "source": [
    "#lendo arquivo com atributos (jeito oficial de armazenar metadados)\n",
    "hdf = h5py.File('hdf5_data_metadata.h5', mode = 'r')\n",
    "ls = list(hdf.keys())\n",
    "print('Lista de Items no Root:\\n', ls,'\\n')\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "#lendo atributos\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "print(k)\n",
    "print(v)\n",
    "print(list(data.attrs.keys()))\n",
    "print(list(data.attrs.values()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
